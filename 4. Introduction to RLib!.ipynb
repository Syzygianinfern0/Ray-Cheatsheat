{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Successfully installed all the dependencies!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "print(\"Successfully installed all the dependencies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Introduction to Policies with Proximal Policy Optimization\n",
    "PPO works in two phases. In one phase, a large number of rollouts are performed (in parallel). The rollouts \n",
    "are then aggregated on the driver and a surrogate optimization objective is defined based on those rollouts. \n",
    "We then use SGD to find the policy that maximizes that objective with a penalty term for diverging too much \n",
    "from the current policy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2019-12-21 16:05:25,122\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2019-12-21 16:05:25,124\tINFO resource_spec.py:216 -- Starting Ray with 8.15 GiB memory available for workers and up to 4.08 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "{'node_ip_address': '192.168.42.189',\n 'redis_address': '192.168.42.189:64105',\n 'object_store_address': '/tmp/ray/session_2019-12-21_16-05-25_121690_12015/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2019-12-21_16-05-25_121690_12015/sockets/raylet',\n 'webui_url': None,\n 'session_dir': '/tmp/ray/session_2019-12-21_16-05-25_121690_12015'}"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "# Start up Ray. This must be done before we instantiate any RL agents.\n",
    "ray.init(num_cpus=3, ignore_reinit_error=True, log_to_driver=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2019-12-21 16:10:16,554\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 3\n",
    "config['num_sgd_iter'] = 15\n",
    "config['sgd_minibatch_size'] = 512\n",
    "config['model']['fcnet_hiddens'] = [32, 32]\n",
    "config['num_cpus_per_worker'] = 0\n",
    "\n",
    "agent = PPOTrainer(config, 'CartPole-v0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-22\n",
      "done: false\n",
      "episode_len_mean: 21.857142857142858\n",
      "episode_reward_max: 74.0\n",
      "episode_reward_mean: 21.857142857142858\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 182\n",
      "episodes_total: 182\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 829.353\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6923392415046692\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0008665229543112218\n",
      "      policy_loss: -0.0025005110073834658\n",
      "      total_loss: 262.7922058105469\n",
      "      vf_explained_var: 3.695487976074219e-05\n",
      "      vf_loss: 262.7945251464844\n",
      "  load_time_ms: 106.892\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 3584\n",
      "  sample_time_ms: 2633.65\n",
      "  update_time_ms: 992.936\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.15555555555555\n",
      "  ram_util_percent: 33.63333333333333\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.11731607712983681\n",
      "  mean_inference_ms: 1.399784664979558\n",
      "  mean_processing_ms: 0.29225583891562995\n",
      "time_since_restore: 4.663139343261719\n",
      "time_this_iter_s: 4.663139343261719\n",
      "time_total_s: 4.663139343261719\n",
      "timestamp: 1576924822\n",
      "timesteps_since_restore: 4000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-25\n",
      "done: false\n",
      "episode_len_mean: 24.81132075471698\n",
      "episode_reward_max: 125.0\n",
      "episode_reward_mean: 24.81132075471698\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 159\n",
      "episodes_total: 341\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 594.644\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6882348656654358\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0017255471320822835\n",
      "      policy_loss: -0.00446103373542428\n",
      "      total_loss: 333.1495666503906\n",
      "      vf_explained_var: 0.00013956001203041524\n",
      "      vf_loss: 333.15380859375\n",
      "  load_time_ms: 54.248\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 7168\n",
      "  sample_time_ms: 2564.528\n",
      "  update_time_ms: 499.591\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 68.85\n",
      "  ram_util_percent: 32.949999999999996\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.11732708484234744\n",
      "  mean_inference_ms: 1.3867938628432872\n",
      "  mean_processing_ms: 0.2868881071015395\n",
      "time_since_restore: 7.534436464309692\n",
      "time_this_iter_s: 2.8712971210479736\n",
      "time_total_s: 7.534436464309692\n",
      "timestamp: 1576924825\n",
      "timesteps_since_restore: 8000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-27\n",
      "done: false\n",
      "episode_len_mean: 28.377622377622377\n",
      "episode_reward_max: 104.0\n",
      "episode_reward_mean: 28.377622377622377\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 143\n",
      "episodes_total: 484\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 520.924\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6778550744056702\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0031135405879467726\n",
      "      policy_loss: -0.010545166209340096\n",
      "      total_loss: 391.8609313964844\n",
      "      vf_explained_var: 0.00047265630564652383\n",
      "      vf_loss: 391.8713684082031\n",
      "  load_time_ms: 36.747\n",
      "  num_steps_sampled: 12000\n",
      "  num_steps_trained: 10752\n",
      "  sample_time_ms: 2533.945\n",
      "  update_time_ms: 335.702\n",
      "iterations_since_restore: 3\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 42.8\n",
      "  ram_util_percent: 33.1\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.11888959507180102\n",
      "  mean_inference_ms: 1.3683859702177814\n",
      "  mean_processing_ms: 0.2844349319904929\n",
      "time_since_restore: 10.402486085891724\n",
      "time_this_iter_s: 2.8680496215820312\n",
      "time_total_s: 10.402486085891724\n",
      "timestamp: 1576924827\n",
      "timesteps_since_restore: 12000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-30\n",
      "done: false\n",
      "episode_len_mean: 30.244274809160306\n",
      "episode_reward_max: 87.0\n",
      "episode_reward_mean: 30.244274809160306\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 131\n",
      "episodes_total: 615\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 486.452\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6603476405143738\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.004180396441370249\n",
      "      policy_loss: -0.004893739242106676\n",
      "      total_loss: 337.9936218261719\n",
      "      vf_explained_var: -0.0010532225714996457\n",
      "      vf_loss: 337.9983825683594\n",
      "  load_time_ms: 28.68\n",
      "  num_steps_sampled: 16000\n",
      "  num_steps_trained: 14336\n",
      "  sample_time_ms: 2534.659\n",
      "  update_time_ms: 253.549\n",
      "iterations_since_restore: 4\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 54.525000000000006\n",
      "  ram_util_percent: 33.1\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.11924042664594041\n",
      "  mean_inference_ms: 1.3715899970626628\n",
      "  mean_processing_ms: 0.2832290782859164\n",
      "time_since_restore: 13.343676090240479\n",
      "time_this_iter_s: 2.941190004348755\n",
      "time_total_s: 13.343676090240479\n",
      "timestamp: 1576924830\n",
      "timesteps_since_restore: 16000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-33\n",
      "done: false\n",
      "episode_len_mean: 36.20909090909091\n",
      "episode_reward_max: 105.0\n",
      "episode_reward_mean: 36.20909090909091\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 110\n",
      "episodes_total: 725\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 462.544\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6457957029342651\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0027692466974258423\n",
      "      policy_loss: -0.011567413806915283\n",
      "      total_loss: 491.81939697265625\n",
      "      vf_explained_var: 0.00010544061660766602\n",
      "      vf_loss: 491.8309631347656\n",
      "  load_time_ms: 23.299\n",
      "  num_steps_sampled: 20000\n",
      "  num_steps_trained: 17920\n",
      "  sample_time_ms: 2547.986\n",
      "  update_time_ms: 203.862\n",
      "iterations_since_restore: 5\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.449999999999996\n",
      "  ram_util_percent: 33.1\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12046738969529158\n",
      "  mean_inference_ms: 1.3846229331747768\n",
      "  mean_processing_ms: 0.28402864032575154\n",
      "time_since_restore: 16.328510761260986\n",
      "time_this_iter_s: 2.984834671020508\n",
      "time_total_s: 16.328510761260986\n",
      "timestamp: 1576924833\n",
      "timesteps_since_restore: 20000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-36\n",
      "done: false\n",
      "episode_len_mean: 42.83\n",
      "episode_reward_max: 126.0\n",
      "episode_reward_mean: 42.83\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 93\n",
      "episodes_total: 818\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 448.703\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6362176537513733\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0013863943750038743\n",
      "      policy_loss: -0.008810841478407383\n",
      "      total_loss: 672.7259521484375\n",
      "      vf_explained_var: 0.001003963639959693\n",
      "      vf_loss: 672.7347412109375\n",
      "  load_time_ms: 19.776\n",
      "  num_steps_sampled: 24000\n",
      "  num_steps_trained: 21504\n",
      "  sample_time_ms: 2545.176\n",
      "  update_time_ms: 171.033\n",
      "iterations_since_restore: 6\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 50.75\n",
      "  ram_util_percent: 33.1\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.11977199358149097\n",
      "  mean_inference_ms: 1.3842485915763272\n",
      "  mean_processing_ms: 0.28153148336154554\n",
      "time_since_restore: 19.25489640235901\n",
      "time_this_iter_s: 2.9263856410980225\n",
      "time_total_s: 19.25489640235901\n",
      "timestamp: 1576924836\n",
      "timesteps_since_restore: 24000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-39\n",
      "done: false\n",
      "episode_len_mean: 43.59\n",
      "episode_reward_max: 120.0\n",
      "episode_reward_mean: 43.59\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 85\n",
      "episodes_total: 903\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 437.601\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6250032782554626\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0012351371115073562\n",
      "      policy_loss: -0.006325642112642527\n",
      "      total_loss: 620.3405151367188\n",
      "      vf_explained_var: 0.002580199856311083\n",
      "      vf_loss: 620.3468627929688\n",
      "  load_time_ms: 17.203\n",
      "  num_steps_sampled: 28000\n",
      "  num_steps_trained: 25088\n",
      "  sample_time_ms: 2544.861\n",
      "  update_time_ms: 147.785\n",
      "iterations_since_restore: 7\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.080000000000005\n",
      "  ram_util_percent: 32.88000000000001\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.11984046453890439\n",
      "  mean_inference_ms: 1.3875898971756697\n",
      "  mean_processing_ms: 0.2801566730522833\n",
      "time_since_restore: 22.188654899597168\n",
      "time_this_iter_s: 2.933758497238159\n",
      "time_total_s: 22.188654899597168\n",
      "timestamp: 1576924839\n",
      "timesteps_since_restore: 28000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-42\n",
      "done: false\n",
      "episode_len_mean: 49.62\n",
      "episode_reward_max: 134.0\n",
      "episode_reward_mean: 49.62\n",
      "episode_reward_min: 14.0\n",
      "episodes_this_iter: 79\n",
      "episodes_total: 982\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 429.604\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6103262901306152\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0014343718066811562\n",
      "      policy_loss: -0.005273669958114624\n",
      "      total_loss: 702.1483764648438\n",
      "      vf_explained_var: 0.00455321604385972\n",
      "      vf_loss: 702.1536865234375\n",
      "  load_time_ms: 15.502\n",
      "  num_steps_sampled: 32000\n",
      "  num_steps_trained: 28672\n",
      "  sample_time_ms: 2554.327\n",
      "  update_time_ms: 130.021\n",
      "iterations_since_restore: 8\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 73.05\n",
      "  ram_util_percent: 32.9\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.11996191676104657\n",
      "  mean_inference_ms: 1.3937564204481765\n",
      "  mean_processing_ms: 0.2795229594272402\n",
      "time_since_restore: 25.199889659881592\n",
      "time_this_iter_s: 3.011234760284424\n",
      "time_total_s: 25.199889659881592\n",
      "timestamp: 1576924842\n",
      "timesteps_since_restore: 32000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-45\n",
      "done: false\n",
      "episode_len_mean: 55.09\n",
      "episode_reward_max: 179.0\n",
      "episode_reward_mean: 55.09\n",
      "episode_reward_min: 16.0\n",
      "episodes_this_iter: 72\n",
      "episodes_total: 1054\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 424.524\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6091161370277405\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00047869974514469504\n",
      "      policy_loss: -0.005800698418170214\n",
      "      total_loss: 830.2421875\n",
      "      vf_explained_var: 0.0019973942544311285\n",
      "      vf_loss: 830.2479858398438\n",
      "  load_time_ms: 14.155\n",
      "  num_steps_sampled: 36000\n",
      "  num_steps_trained: 32256\n",
      "  sample_time_ms: 2559.107\n",
      "  update_time_ms: 116.209\n",
      "iterations_since_restore: 9\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.775\n",
      "  ram_util_percent: 33.1\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12020185560026345\n",
      "  mean_inference_ms: 1.3988982298905972\n",
      "  mean_processing_ms: 0.2789629563760905\n",
      "time_since_restore: 28.20009469985962\n",
      "time_this_iter_s: 3.0002050399780273\n",
      "time_total_s: 28.20009469985962\n",
      "timestamp: 1576924845\n",
      "timesteps_since_restore: 36000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-48\n",
      "done: false\n",
      "episode_len_mean: 64.13\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 64.13\n",
      "episode_reward_min: 16.0\n",
      "episodes_this_iter: 58\n",
      "episodes_total: 1112\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 420.802\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6096020340919495\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0006563884089700878\n",
      "      policy_loss: 0.0015996939036995173\n",
      "      total_loss: 1015.9122314453125\n",
      "      vf_explained_var: 0.006558239459991455\n",
      "      vf_loss: 1015.91064453125\n",
      "  load_time_ms: 12.976\n",
      "  num_steps_sampled: 40000\n",
      "  num_steps_trained: 35840\n",
      "  sample_time_ms: 2560.775\n",
      "  update_time_ms: 105.157\n",
      "iterations_since_restore: 10\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 65.32000000000001\n",
      "  ram_util_percent: 33.1\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12034084321283618\n",
      "  mean_inference_ms: 1.401690966274609\n",
      "  mean_processing_ms: 0.2785581251585064\n",
      "time_since_restore: 31.17659592628479\n",
      "time_this_iter_s: 2.976501226425171\n",
      "time_total_s: 31.17659592628479\n",
      "timestamp: 1576924848\n",
      "timesteps_since_restore: 40000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-52\n",
      "done: false\n",
      "episode_len_mean: 69.53\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 69.53\n",
      "episode_reward_min: 16.0\n",
      "episodes_this_iter: 57\n",
      "episodes_total: 1169\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 375.199\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6037641167640686\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0009263715473935008\n",
      "      policy_loss: -0.009403051808476448\n",
      "      total_loss: 961.25927734375\n",
      "      vf_explained_var: 0.009192466735839844\n",
      "      vf_loss: 961.2688598632812\n",
      "  load_time_ms: 2.437\n",
      "  num_steps_sampled: 44000\n",
      "  num_steps_trained: 39424\n",
      "  sample_time_ms: 2581.892\n",
      "  update_time_ms: 6.396\n",
      "iterations_since_restore: 11\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.125\n",
      "  ram_util_percent: 33.05\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12074620034555295\n",
      "  mean_inference_ms: 1.4053464177891215\n",
      "  mean_processing_ms: 0.27760438506217144\n",
      "time_since_restore: 34.40983057022095\n",
      "time_this_iter_s: 3.2332346439361572\n",
      "time_total_s: 34.40983057022095\n",
      "timestamp: 1576924852\n",
      "timesteps_since_restore: 44000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 44000\n",
      "training_iteration: 11\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-54\n",
      "done: false\n",
      "episode_len_mean: 75.72\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 75.72\n",
      "episode_reward_min: 14.0\n",
      "episodes_this_iter: 49\n",
      "episodes_total: 1218\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 374.72\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5990455746650696\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0025726722087711096\n",
      "      policy_loss: 0.0009278910583816469\n",
      "      total_loss: 1304.5933837890625\n",
      "      vf_explained_var: 0.00837699044495821\n",
      "      vf_loss: 1304.5921630859375\n",
      "  load_time_ms: 2.528\n",
      "  num_steps_sampled: 48000\n",
      "  num_steps_trained: 43008\n",
      "  sample_time_ms: 2579.739\n",
      "  update_time_ms: 6.238\n",
      "iterations_since_restore: 12\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.8\n",
      "  ram_util_percent: 33.05\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12095172412408091\n",
      "  mean_inference_ms: 1.405609575872878\n",
      "  mean_processing_ms: 0.2763805548000776\n",
      "time_since_restore: 37.255924463272095\n",
      "time_this_iter_s: 2.8460938930511475\n",
      "time_total_s: 37.255924463272095\n",
      "timestamp: 1576924854\n",
      "timesteps_since_restore: 48000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 48000\n",
      "training_iteration: 12\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-10-57\n",
      "done: false\n",
      "episode_len_mean: 84.97\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 84.97\n",
      "episode_reward_min: 14.0\n",
      "episodes_this_iter: 43\n",
      "episodes_total: 1261\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 375.588\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5939193367958069\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0010433349525555968\n",
      "      policy_loss: 0.001829089829698205\n",
      "      total_loss: 1521.458984375\n",
      "      vf_explained_var: 0.007128136698156595\n",
      "      vf_loss: 1521.45703125\n",
      "  load_time_ms: 2.68\n",
      "  num_steps_sampled: 52000\n",
      "  num_steps_trained: 46592\n",
      "  sample_time_ms: 2592.399\n",
      "  update_time_ms: 6.301\n",
      "iterations_since_restore: 13\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.86\n",
      "  ram_util_percent: 32.84\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12086651241473906\n",
      "  mean_inference_ms: 1.4040313874240002\n",
      "  mean_processing_ms: 0.2748026793650436\n",
      "time_since_restore: 40.25856375694275\n",
      "time_this_iter_s: 3.0026392936706543\n",
      "time_total_s: 40.25856375694275\n",
      "timestamp: 1576924857\n",
      "timesteps_since_restore: 52000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 52000\n",
      "training_iteration: 13\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-00\n",
      "done: false\n",
      "episode_len_mean: 97.84\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 97.84\n",
      "episode_reward_min: 19.0\n",
      "episodes_this_iter: 34\n",
      "episodes_total: 1295\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 374.382\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5928574800491333\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0005839316872879863\n",
      "      policy_loss: 0.011073313653469086\n",
      "      total_loss: 1599.703369140625\n",
      "      vf_explained_var: 0.021705253049731255\n",
      "      vf_loss: 1599.6925048828125\n",
      "  load_time_ms: 2.606\n",
      "  num_steps_sampled: 56000\n",
      "  num_steps_trained: 50176\n",
      "  sample_time_ms: 2591.935\n",
      "  update_time_ms: 6.254\n",
      "iterations_since_restore: 14\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.75\n",
      "  ram_util_percent: 32.825\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.1207059316247258\n",
      "  mean_inference_ms: 1.404052271906965\n",
      "  mean_processing_ms: 0.2735486070028212\n",
      "time_since_restore: 43.18122577667236\n",
      "time_this_iter_s: 2.9226620197296143\n",
      "time_total_s: 43.18122577667236\n",
      "timestamp: 1576924860\n",
      "timesteps_since_restore: 56000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 56000\n",
      "training_iteration: 14\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-03\n",
      "done: false\n",
      "episode_len_mean: 111.69\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 111.69\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 31\n",
      "episodes_total: 1326\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 375.218\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5924890637397766\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0006485971389338374\n",
      "      policy_loss: -0.004575646016746759\n",
      "      total_loss: 1842.0482177734375\n",
      "      vf_explained_var: 0.015773696824908257\n",
      "      vf_loss: 1842.0528564453125\n",
      "  load_time_ms: 2.819\n",
      "  num_steps_sampled: 60000\n",
      "  num_steps_trained: 53760\n",
      "  sample_time_ms: 2590.017\n",
      "  update_time_ms: 6.647\n",
      "iterations_since_restore: 15\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 65.325\n",
      "  ram_util_percent: 33.0\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12101018337010462\n",
      "  mean_inference_ms: 1.4071409567140034\n",
      "  mean_processing_ms: 0.27303578263576805\n",
      "time_since_restore: 46.16077661514282\n",
      "time_this_iter_s: 2.979550838470459\n",
      "time_total_s: 46.16077661514282\n",
      "timestamp: 1576924863\n",
      "timesteps_since_restore: 60000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 60000\n",
      "training_iteration: 15\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-06\n",
      "done: false\n",
      "episode_len_mean: 125.58\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 125.58\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 27\n",
      "episodes_total: 1353\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 374.874\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5799736380577087\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0026353627908974886\n",
      "      policy_loss: -0.0002800244837999344\n",
      "      total_loss: 1767.353759765625\n",
      "      vf_explained_var: 0.011933863162994385\n",
      "      vf_loss: 1767.3543701171875\n",
      "  load_time_ms: 2.81\n",
      "  num_steps_sampled: 64000\n",
      "  num_steps_trained: 57344\n",
      "  sample_time_ms: 2589.705\n",
      "  update_time_ms: 6.521\n",
      "iterations_since_restore: 16\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 52.725\n",
      "  ram_util_percent: 33.0\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12111265682832727\n",
      "  mean_inference_ms: 1.407816214945864\n",
      "  mean_processing_ms: 0.2717360487867614\n",
      "time_since_restore: 49.08040642738342\n",
      "time_this_iter_s: 2.9196298122406006\n",
      "time_total_s: 49.08040642738342\n",
      "timestamp: 1576924866\n",
      "timesteps_since_restore: 64000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 64000\n",
      "training_iteration: 16\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-09\n",
      "done: false\n",
      "episode_len_mean: 133.33\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 133.33\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 1383\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 375.542\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5742044448852539\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0037569659762084484\n",
      "      policy_loss: -0.003440887201577425\n",
      "      total_loss: 1583.1898193359375\n",
      "      vf_explained_var: 0.021119236946105957\n",
      "      vf_loss: 1583.193359375\n",
      "  load_time_ms: 3.065\n",
      "  num_steps_sampled: 68000\n",
      "  num_steps_trained: 60928\n",
      "  sample_time_ms: 2590.499\n",
      "  update_time_ms: 6.195\n",
      "iterations_since_restore: 17\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.95\n",
      "  ram_util_percent: 33.0\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12109319175379268\n",
      "  mean_inference_ms: 1.4079860097608952\n",
      "  mean_processing_ms: 0.27025018575919035\n",
      "time_since_restore: 52.027974128723145\n",
      "time_this_iter_s: 2.9475677013397217\n",
      "time_total_s: 52.027974128723145\n",
      "timestamp: 1576924869\n",
      "timesteps_since_restore: 68000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 68000\n",
      "training_iteration: 17\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-12\n",
      "done: false\n",
      "episode_len_mean: 144.0\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 144.0\n",
      "episode_reward_min: 22.0\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 1407\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 376.518\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5903279185295105\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0006505381898023188\n",
      "      policy_loss: 0.0033813384361565113\n",
      "      total_loss: 1768.1280517578125\n",
      "      vf_explained_var: 0.03262687474489212\n",
      "      vf_loss: 1768.124755859375\n",
      "  load_time_ms: 2.933\n",
      "  num_steps_sampled: 72000\n",
      "  num_steps_trained: 64512\n",
      "  sample_time_ms: 2587.358\n",
      "  update_time_ms: 6.262\n",
      "iterations_since_restore: 18\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.160000000000004\n",
      "  ram_util_percent: 33.059999999999995\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12109202367588473\n",
      "  mean_inference_ms: 1.4088422000746283\n",
      "  mean_processing_ms: 0.26933957952646315\n",
      "time_since_restore: 55.016366720199585\n",
      "time_this_iter_s: 2.9883925914764404\n",
      "time_total_s: 55.016366720199585\n",
      "timestamp: 1576924872\n",
      "timesteps_since_restore: 72000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 72000\n",
      "training_iteration: 18\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-15\n",
      "done: false\n",
      "episode_len_mean: 150.27\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 150.27\n",
      "episode_reward_min: 22.0\n",
      "episodes_this_iter: 26\n",
      "episodes_total: 1433\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 376.826\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5782831311225891\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00026562545099295676\n",
      "      policy_loss: -0.007491058669984341\n",
      "      total_loss: 1800.6026611328125\n",
      "      vf_explained_var: 0.01844138838350773\n",
      "      vf_loss: 1800.6102294921875\n",
      "  load_time_ms: 3.013\n",
      "  num_steps_sampled: 76000\n",
      "  num_steps_trained: 68096\n",
      "  sample_time_ms: 2585.487\n",
      "  update_time_ms: 6.462\n",
      "iterations_since_restore: 19\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 61.25\n",
      "  ram_util_percent: 32.925\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12120237549125112\n",
      "  mean_inference_ms: 1.4102764398765282\n",
      "  mean_processing_ms: 0.2685018828022041\n",
      "time_since_restore: 58.00313663482666\n",
      "time_this_iter_s: 2.986769914627075\n",
      "time_total_s: 58.00313663482666\n",
      "timestamp: 1576924875\n",
      "timesteps_since_restore: 76000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 76000\n",
      "training_iteration: 19\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-18\n",
      "done: false\n",
      "episode_len_mean: 151.35\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 151.35\n",
      "episode_reward_min: 33.0\n",
      "episodes_this_iter: 25\n",
      "episodes_total: 1458\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 374.672\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5861607193946838\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0016830855747684836\n",
      "      policy_loss: -0.002791831037029624\n",
      "      total_loss: 1883.838623046875\n",
      "      vf_explained_var: 0.021963289007544518\n",
      "      vf_loss: 1883.8414306640625\n",
      "  load_time_ms: 3.146\n",
      "  num_steps_sampled: 80000\n",
      "  num_steps_trained: 71680\n",
      "  sample_time_ms: 2582.887\n",
      "  update_time_ms: 6.557\n",
      "iterations_since_restore: 20\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.275\n",
      "  ram_util_percent: 32.85\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12133751753337815\n",
      "  mean_inference_ms: 1.411185487851308\n",
      "  mean_processing_ms: 0.2677050115137924\n",
      "time_since_restore: 60.93724274635315\n",
      "time_this_iter_s: 2.9341061115264893\n",
      "time_total_s: 60.93724274635315\n",
      "timestamp: 1576924878\n",
      "timesteps_since_restore: 80000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 80000\n",
      "training_iteration: 20\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-21\n",
      "done: false\n",
      "episode_len_mean: 161.57\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 161.57\n",
      "episode_reward_min: 40.0\n",
      "episodes_this_iter: 24\n",
      "episodes_total: 1482\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 374.578\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5783557891845703\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0017173931701108813\n",
      "      policy_loss: -0.0027987852226942778\n",
      "      total_loss: 2119.14306640625\n",
      "      vf_explained_var: 0.017140107229351997\n",
      "      vf_loss: 2119.1455078125\n",
      "  load_time_ms: 3.369\n",
      "  num_steps_sampled: 84000\n",
      "  num_steps_trained: 75264\n",
      "  sample_time_ms: 2550.304\n",
      "  update_time_ms: 6.575\n",
      "iterations_since_restore: 21\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.35\n",
      "  ram_util_percent: 33.074999999999996\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12158069225774193\n",
      "  mean_inference_ms: 1.4126313764834095\n",
      "  mean_processing_ms: 0.26720792989203823\n",
      "time_since_restore: 63.84742712974548\n",
      "time_this_iter_s: 2.910184383392334\n",
      "time_total_s: 63.84742712974548\n",
      "timestamp: 1576924881\n",
      "timesteps_since_restore: 84000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-24\n",
      "done: false\n",
      "episode_len_mean: 163.38\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 163.38\n",
      "episode_reward_min: 40.0\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 1505\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 377.96\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5745090246200562\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00027604555361904204\n",
      "      policy_loss: -0.006341446656733751\n",
      "      total_loss: 2139.066162109375\n",
      "      vf_explained_var: 0.010095136240124702\n",
      "      vf_loss: 2139.072509765625\n",
      "  load_time_ms: 3.456\n",
      "  num_steps_sampled: 88000\n",
      "  num_steps_trained: 78848\n",
      "  sample_time_ms: 2557.286\n",
      "  update_time_ms: 6.565\n",
      "iterations_since_restore: 22\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 68.14\n",
      "  ram_util_percent: 33.160000000000004\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12157530446175878\n",
      "  mean_inference_ms: 1.4126922709367449\n",
      "  mean_processing_ms: 0.26650357717292367\n",
      "time_since_restore: 66.80156517028809\n",
      "time_this_iter_s: 2.9541380405426025\n",
      "time_total_s: 66.80156517028809\n",
      "timestamp: 1576924884\n",
      "timesteps_since_restore: 88000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 88000\n",
      "training_iteration: 22\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-27\n",
      "done: false\n",
      "episode_len_mean: 166.63\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 166.63\n",
      "episode_reward_min: 40.0\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 1528\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 377.555\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5654016733169556\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.000495227228384465\n",
      "      policy_loss: 0.003994246479123831\n",
      "      total_loss: 1777.745849609375\n",
      "      vf_explained_var: 0.01628183387219906\n",
      "      vf_loss: 1777.7418212890625\n",
      "  load_time_ms: 3.341\n",
      "  num_steps_sampled: 92000\n",
      "  num_steps_trained: 82432\n",
      "  sample_time_ms: 2545.173\n",
      "  update_time_ms: 6.143\n",
      "iterations_since_restore: 23\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.925\n",
      "  ram_util_percent: 33.2\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12162384364815533\n",
      "  mean_inference_ms: 1.412177793959741\n",
      "  mean_processing_ms: 0.26570433744088506\n",
      "time_since_restore: 69.67336106300354\n",
      "time_this_iter_s: 2.871795892715454\n",
      "time_total_s: 69.67336106300354\n",
      "timestamp: 1576924887\n",
      "timesteps_since_restore: 92000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 92000\n",
      "training_iteration: 23\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-30\n",
      "done: false\n",
      "episode_len_mean: 171.32\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 171.32\n",
      "episode_reward_min: 40.0\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 1550\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 378.133\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5674923658370972\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00036984565667808056\n",
      "      policy_loss: 0.012112299911677837\n",
      "      total_loss: 1750.5770263671875\n",
      "      vf_explained_var: 0.010288715362548828\n",
      "      vf_loss: 1750.5648193359375\n",
      "  load_time_ms: 3.343\n",
      "  num_steps_sampled: 96000\n",
      "  num_steps_trained: 86016\n",
      "  sample_time_ms: 2545.988\n",
      "  update_time_ms: 5.854\n",
      "iterations_since_restore: 24\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 53.325\n",
      "  ram_util_percent: 33.3\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12156377314391381\n",
      "  mean_inference_ms: 1.411420639320583\n",
      "  mean_processing_ms: 0.2649159983509559\n",
      "time_since_restore: 72.60628294944763\n",
      "time_this_iter_s: 2.932921886444092\n",
      "time_total_s: 72.60628294944763\n",
      "timestamp: 1576924890\n",
      "timesteps_since_restore: 96000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 96000\n",
      "training_iteration: 24\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-12-21_16-11-33\n",
      "done: false\n",
      "episode_len_mean: 177.97\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 177.97\n",
      "episode_reward_min: 61.0\n",
      "episodes_this_iter: 23\n",
      "episodes_total: 1573\n",
      "experiment_id: 2927b54e348a4ddb95bcfba414f8e925\n",
      "hostname: azkaban\n",
      "info:\n",
      "  grad_time_ms: 377.306\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5586380362510681\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00041720064473338425\n",
      "      policy_loss: 0.00019795527623500675\n",
      "      total_loss: 2156.158935546875\n",
      "      vf_explained_var: 0.01959346793591976\n",
      "      vf_loss: 2156.158935546875\n",
      "  load_time_ms: 3.225\n",
      "  num_steps_sampled: 100000\n",
      "  num_steps_trained: 89600\n",
      "  sample_time_ms: 2531.997\n",
      "  update_time_ms: 5.726\n",
      "iterations_since_restore: 25\n",
      "node_ip: 192.168.42.189\n",
      "num_healthy_workers: 3\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 51.400000000000006\n",
      "  ram_util_percent: 33.199999999999996\n",
      "pid: 12015\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.12151300809076127\n",
      "  mean_inference_ms: 1.410612585111763\n",
      "  mean_processing_ms: 0.2641266986300127\n",
      "time_since_restore: 75.43531012535095\n",
      "time_this_iter_s: 2.8290271759033203\n",
      "time_total_s: 75.43531012535095\n",
      "timestamp: 1576924893\n",
      "timesteps_since_restore: 100000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 100000\n",
      "training_iteration: 25\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    result = agent.train()\n",
    "    print(pretty_print(result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "/home/syzygianinfern0/ray_results/PPO_CartPole-v0_2019-12-21_16-10-10sbwmgs0a/checkpoint_25/checkpoint-25\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "checkpoint_path = agent.save()\n",
    "print(checkpoint_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2019-12-21 16:25:41,549\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "2019-12-21 16:25:41,665\tINFO trainable.py:346 -- Restored from checkpoint: /home/syzygianinfern0/ray_results/PPO_CartPole-v0_2019-12-21_16-10-10sbwmgs0a/checkpoint_25/checkpoint-25\n",
      "2019-12-21 16:25:41,667\tINFO trainable.py:353 -- Current state after restoring: {'_iteration': 25, '_timesteps_total': 100000, '_time_total': 75.43531012535095, '_episodes_total': 1573}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "trained_config = config.copy()\n",
    "\n",
    "test_agent = PPOTrainer(trained_config, 'CartPole-v0')\n",
    "test_agent.restore(checkpoint_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "200.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = test_agent.compute_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(cumulative_reward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Custom Environments and Rewards Shaping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import numpy as np\n",
    "import test_exercises"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2019-12-21 16:32:09,517\tERROR worker.py:679 -- Calling ray.init() again after it has already been called.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True, log_to_driver=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Success!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "action_space_map = {\n",
    "    \"discrete_10\": spaces.Discrete(10),\n",
    "    \"box_1\": spaces.Box(0, 1, shape=(1,)),\n",
    "    \"box_3x1\": spaces.Box(-2, 2, shape=(3, 1)),\n",
    "    \"multi_discrete\": spaces.MultiDiscrete([ 5, 2, 2, 4 ])\n",
    "}\n",
    "\n",
    "action_space_jumble = {\n",
    "    \"discrete_10\": 1,\n",
    "    \"multi_discrete\": np.array([0, 0, 0, 2]),\n",
    "    \"box_3x1\": np.array([[-1.2657754], [-1.6528835], [ 0.5982418]]),\n",
    "    \"box_1\": np.array([0.89089584]),\n",
    "}\n",
    "\n",
    "\n",
    "for space_id, state in action_space_jumble.items():\n",
    "    assert action_space_map[space_id].contains(state), (\n",
    "        \"Looks like {} to {} is matched incorrectly.\".format(space_id, state))\n",
    "    \n",
    "print(\"Success!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Testing if spaces have been setup correctly...\n",
      "Success! You've setup the spaces correctly.\n",
      "Testing if reward has been setup correctly...\n",
      "Success! You've setup the rewards correctly.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class ChainEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, env_config = None):\n",
    "        env_config = env_config or {}\n",
    "        self.n = env_config.get(\"n\", 20)\n",
    "        self.small_reward = env_config.get(\"small\", 2)  # payout for 'backwards' action\n",
    "        self.large_reward = env_config.get(\"large\", 10)  # payout at end of chain for 'forwards' action\n",
    "        self.state = 0  # Start at beginning of the chain\n",
    "        self._horizon = self.n\n",
    "        self._counter = 0  # For terminating the episode\n",
    "        self._setup_spaces()\n",
    "    \n",
    "    def _setup_spaces(self):\n",
    "        ##############\n",
    "        # TODO: Implement this so that it passes tests\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Discrete(self.n)\n",
    "        ##############\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:  # 'backwards': go back to the beginning, get small reward\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "            reward = self.small_reward\n",
    "            ##############\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "            reward = 0\n",
    "            self.state += 1\n",
    "        else:  # 'forwards': stay at the end of the chain, collect large reward\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "            reward = self.large_reward\n",
    "            ##############\n",
    "        self._counter += 1\n",
    "        done = self._counter >= self._horizon\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self._counter = 0\n",
    "        return self.state\n",
    "    \n",
    "# Tests here:\n",
    "test_exercises.test_chain_env_spaces(ChainEnv)\n",
    "test_exercises.test_chain_env_reward(ChainEnv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()\n",
    "trainer_config['num_workers'] = 1\n",
    "trainer_config[\"train_batch_size\"] = 400\n",
    "trainer_config[\"sgd_minibatch_size\"] = 64\n",
    "trainer_config[\"num_sgd_iter\"] = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2019-12-21 16:36:26,921\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Training iteration 0...\n",
      "Training iteration 1...\n",
      "Training iteration 2...\n",
      "Training iteration 3...\n",
      "Training iteration 4...\n",
      "Training iteration 5...\n",
      "Training iteration 6...\n",
      "Training iteration 7...\n",
      "Training iteration 8...\n",
      "Training iteration 9...\n",
      "Training iteration 10...\n",
      "Training iteration 11...\n",
      "Training iteration 12...\n",
      "Training iteration 13...\n",
      "Training iteration 14...\n",
      "Training iteration 15...\n",
      "Training iteration 16...\n",
      "Training iteration 17...\n",
      "Training iteration 18...\n",
      "Training iteration 19...\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "trainer = PPOTrainer(trainer_config, ChainEnv)\n",
    "for i in range(20):\n",
    "    print(\"Training iteration {}...\".format(i))\n",
    "    trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Cumulative reward you've received is: 40. Congratulations!\n",
      "Max state you've visited is: 0. This is out of 20 states.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "env = ChainEnv({})\n",
    "state = env.reset()\n",
    "\n",
    "done = False\n",
    "max_state = -1\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_action(state)\n",
    "    state, reward, done, results = env.step(action)\n",
    "    max_state = max(max_state, state)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(\"Cumulative reward you've received is: {}. Congratulations!\".format(cumulative_reward))\n",
    "print(\"Max state you've visited is: {}. This is out of {} states.\".format(max_state, env.n))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Shaping the rewards to encourage proper behaviour"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Testing if behavior has been changed...\n",
      "Success! Behavior of environment is correct.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "class ShapedChainEnv(ChainEnv):\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:  # 'backwards': go back to the beginning\n",
    "            reward = -20 * self.large_reward\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
    "            reward = self.small_reward\n",
    "            self.state += 1\n",
    "        else:  # 'forwards': stay at the end of the chain\n",
    "            reward = 20 * self.large_reward\n",
    "        self._counter += 1\n",
    "        done = self._counter >= self._horizon\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "test_exercises.test_chain_env_behavior(ShapedChainEnv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "2019-12-21 16:40:38,997\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "2019-12-21 16:40:41,848\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 211.0x the scale of `vf_clip_param`. This means that it will take more than 211.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "2019-12-21 16:40:42,668\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "2019-12-21 16:40:43,586\tWARNING ppo.py:144 -- The magnitude of your environment rewards are more than 203.0x the scale of `vf_clip_param`. This means that it will take more than 203.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Training iteration 0...\n",
      "Training iteration 1...\n",
      "Training iteration 2...\n",
      "Training iteration 3...\n",
      "Training iteration 4...\n",
      "Training iteration 5...\n",
      "Training iteration 6...\n",
      "Training iteration 7...\n",
      "Training iteration 8...\n",
      "Training iteration 9...\n",
      "Training iteration 10...\n",
      "Training iteration 11...\n",
      "Training iteration 12...\n",
      "Training iteration 13...\n",
      "Training iteration 14...\n",
      "Training iteration 15...\n",
      "Training iteration 16...\n",
      "Training iteration 17...\n",
      "Training iteration 18...\n",
      "Training iteration 19...\n",
      "Cumulative reward you've received is: -566!\n",
      "Max state you've visited is: 7.0. This is out of 20 states.\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-184ae9d1e9a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cumulative reward you've received is: {}!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulative_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max state you've visited is: {}. This is out of {} states.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"This policy did not traverse many states.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: This policy did not traverse many states."
     ],
     "ename": "AssertionError",
     "evalue": "This policy did not traverse many states.",
     "output_type": "error"
    }
   ],
   "source": [
    "trainer = PPOTrainer(trainer_config, ShapedChainEnv)\n",
    "for i in range(20):\n",
    "    print(\"Training iteration {}...\".format(i))\n",
    "    trainer.train()\n",
    "\n",
    "env = ShapedChainEnv({})\n",
    "\n",
    "max_states = []\n",
    "\n",
    "for i in range(5):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    max_state = -1\n",
    "    cumulative_reward = 0\n",
    "    while not done:\n",
    "        action = trainer.compute_action(state)\n",
    "        state, reward, done, results = env.step(action)\n",
    "        max_state = max(max_state, state)\n",
    "        cumulative_reward += reward\n",
    "    max_states += [max_state]\n",
    "\n",
    "print(\"Cumulative reward you've received is: {}!\".format(cumulative_reward))\n",
    "print(\"Max state you've visited is: {}. This is out of {} states.\".format(np.mean(max_state), env.n))\n",
    "assert (env.n - np.mean(max_state)) / env.n < 0.2, \"This policy did not traverse many states.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}